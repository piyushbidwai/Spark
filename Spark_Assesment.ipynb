{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad78eefb-1bf2-40de-9eea-1d033e86e7ef",
   "metadata": {},
   "source": [
    "**Setting up necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dde976-3fef-466e-8e14-8267a7142dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pyspark in f:\\data engineering\\pyspark\\my_venv\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in f:\\data engineering\\pyspark\\my_venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: F:\\Data Engineering\\Pyspark\\my_venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e573719-a9c3-4636-834e-d9170dca66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in f:\\data engineering\\pyspark\\my_venv\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: F:\\Data Engineering\\Pyspark\\my_venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d28f5af-e09b-4759-a7f1-93e4d4a8d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d992334a-cdc3-4b54-aa86-8a08bf1ca91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f123446-5a0f-43ea-ae67-41fda9a25ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Spark-Assesment').master('local[2]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a0fb3-4999-40aa-b291-37991af1f828",
   "metadata": {},
   "source": [
    "**Reading the File** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bcab54a3-90c8-463f-83aa-dde94f3874c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header', True).option('inferSchema', True).csv(\"database.csv\")                              # Reading database.csv file in to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "14ea560d-dac0-422b-8664-3a1af73acfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTable = df.createOrReplaceTempView('neic_earthquakes')                   #Creating and storing the csv data into the table 'neic_earthquakes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b57e801-4207-4d46-bb66-ac1d85df5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------+---------+----------+-----+-----------+----------------------+---------+--------------+---------------+--------------------------+-------------+-------------------+----------------+----------------+------------+------+---------------+----------------+---------+\n",
      "|      Date|               Time|Latitude|Longitude|      Type|Depth|Depth Error|Depth Seismic Stations|Magnitude|Magnitude Type|Magnitude Error|Magnitude Seismic Stations|Azimuthal Gap|Horizontal Distance|Horizontal Error|Root Mean Square|          ID|Source|Location Source|Magnitude Source|   Status|\n",
      "+----------+-------------------+--------+---------+----------+-----+-----------+----------------------+---------+--------------+---------------+--------------------------+-------------+-------------------+----------------+----------------+------------+------+---------------+----------------+---------+\n",
      "|01-02-1965|2023-11-22 13:44:18|  19.246|  145.616|Earthquake|131.6|       NULL|                  NULL|      6.0|            MW|           NULL|                      NULL|         NULL|               NULL|            NULL|            NULL|ISCGEM860706|ISCGEM|         ISCGEM|          ISCGEM|Automatic|\n",
      "|01-04-1965|2023-11-22 11:29:49|   1.863|  127.352|Earthquake| 80.0|       NULL|                  NULL|      5.8|            MW|           NULL|                      NULL|         NULL|               NULL|            NULL|            NULL|ISCGEM860737|ISCGEM|         ISCGEM|          ISCGEM|Automatic|\n",
      "+----------+-------------------+--------+---------+----------+-----+-----------+----------------------+---------+--------------+---------------+--------------------------+-------------+-------------------+----------------+----------------+------------+------+---------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM neic_earthquakes limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "43c1421f-cc4c-4e50-9740-08754c33f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"SELECT * from neic_earthquakes\")                 #Reading the data from a table into a Pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "78b61cfe-17d2-41e7-ab3b-a0199f65f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             Type|\n",
      "+-----------------+\n",
      "|        Explosion|\n",
      "|       Rock Burst|\n",
      "|Nuclear Explosion|\n",
      "|       Earthquake|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dd32ade4-d464-436d-a97b-d84ab704f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "df3 = df.withColumn('day_of_week', dayofweek('Time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b7a92fc5-f0fa-4a4f-81a9-f018cb62fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|day_of_week|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select('day_of_week').distinct().show()              #Day of Week 1 == Monday ; 2 == tuesday..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c541d21c-f1ba-4361-9c47-3bda3d89e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = df3.filter(df3.Type == 'Earthquake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c4ae88f7-b7c2-4924-9909-8364c9561b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf2 = tempdf.select('Date', 'day_of_week', 'Type')                    #Selecting Required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e24e4a27-4051-446d-9680-14bffdd3033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|day_of_week|count_of_earthquakes|\n",
      "+-----------+--------------------+\n",
      "|          1|                   3|\n",
      "|          4|               23229|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count                                                      # Day of a Week affecting the number of earthquakes\n",
    "tempdf2.groupBy('day_of_week').agg(count(\"*\").alias('count_of_earthquakes')).show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "329c86a4-655d-4ef8-8c76-9e201e9ed59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *                                  #Extracting Year column and day of month into a pyspark dateframe\n",
    "tempDF = df.withColumn(\"Time\", col(\"Time\").cast(\"timestamp\"))\n",
    "tempDF = tempDF.withColumn(\"Year\", year(\"Time\")).withColumn(\"Day_of_month\", day(\"Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b17067b5-a0e9-4eda-9e9d-38f5858e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:-\n",
    "## What is the relation between Day of the month and Number of earthquakes that happened in a year?\n",
    "\n",
    "day_of_month_result = tempDF.groupBy('Year', 'Day_of_month').agg({'Type':\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "520dba32-015d-4349-b1ed-6ced49ac08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question : What is the relation between Year and Number of earthquakes that happened in that year?\n",
    "yearly_results = tempDF.groupBy(\"Year\").agg({\"Type\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2fbcd31b-9272-4f81-a2a7-06ef4c045ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: How has the earthquake magnitude on average been varied over the years?\n",
    "magnitude_over_years = tempDF.groupBy(\"Year\").agg({\"Magnitude\": \"avg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a0aa88c9-2acd-4066-9199-b3f98754ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question : How does year impact the standard deviation of the earthquakes?\n",
    "stddev_results = tempDF.groupBy('Year').agg(stddev(\"Magnitude\").alias(\"Magnitude_StdDev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e21bfd56-973b-4a34-81d8-2c8228296d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Does geographic location have anything to do with earthquakes?\n",
    "location_results = tempDF.groupBy(\"Location Source\").agg({'Type':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "23136e96-55b0-4706-bf27-e98ea8dfda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: What is the relation between Magnitude, Magnitude Type , Status and Root Mean Square of the earthquakes?\n",
    "relation_status_results = tempDF.groupBy(\"Magnitude Type\", \"Status\").agg({\"Magnitude\":\"avg\", \"Root Mean Square\":\"avg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b14f7fb8-d47e-4c6a-8beb-238989d7dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|   avg(Magnitude)|\n",
      "+----+-----------------+\n",
      "|1975|              5.6|\n",
      "|2023|5.882558417702829|\n",
      "|1985|              5.6|\n",
      "|2011|              5.8|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "magnitude_over_years.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "06f92a23-4e47-4e44-9f49-7267d649b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|  Magnitude_StdDev|\n",
      "+----+------------------+\n",
      "|1975|              NULL|\n",
      "|2023|0.4230843439717061|\n",
      "|1985|              NULL|\n",
      "|2011|              NULL|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stddev_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b8cbf8b2-adb0-414e-abfa-2a4f051c0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|Location Source|count(Type)|\n",
      "+---------------+-----------+\n",
      "|             CI|         61|\n",
      "|            CAR|          1|\n",
      "|            MDD|          2|\n",
      "|            UCR|          1|\n",
      "|           GCMT|         56|\n",
      "|         ISCGEM|       2581|\n",
      "|            OTT|          1|\n",
      "|           RSPR|          3|\n",
      "|           CASC|          4|\n",
      "|             UW|          6|\n",
      "|            ROM|          7|\n",
      "|            TEH|          7|\n",
      "|              B|          2|\n",
      "|            SJA|          1|\n",
      "|            BOU|          1|\n",
      "|           AEIC|         40|\n",
      "|            ATH|         14|\n",
      "|            JMA|          3|\n",
      "|              U|          1|\n",
      "|            TUL|          2|\n",
      "+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7dedf7b4-dc8a-4984-a75c-d5b7b82a8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------------------+---------------------+\n",
      "|Magnitude Type|   Status|    avg(Magnitude)|avg(Root Mean Square)|\n",
      "+--------------+---------+------------------+---------------------+\n",
      "|            ML| Reviewed| 5.814675324675323|  0.46337894736842095|\n",
      "|            MH| Reviewed| 6.540000000000001|  0.11379999999999998|\n",
      "|            MW| Reviewed| 5.896276988912613|   1.0882880094043805|\n",
      "|            MW|Automatic| 6.008523827973685|                 NULL|\n",
      "|           MWC| Reviewed|5.8579087994299375|   1.0121028716830256|\n",
      "|          NULL|Automatic| 5.706666666666666|                 NULL|\n",
      "|            MD| Reviewed| 5.966666666666668|  0.19833333333333333|\n",
      "|           MWC|Automatic| 5.885454545454545|                 NULL|\n",
      "|            MB| Reviewed|  5.68295666046264|    0.989051266941661|\n",
      "|           MWR| Reviewed|  5.63076923076923|   0.9381818181818183|\n",
      "|           MWW| Reviewed| 6.008673726676766|    0.962074795081968|\n",
      "|            MS| Reviewed| 5.994359576968296|    1.105131810193322|\n",
      "|           MWB| Reviewed| 5.907282343368652|   0.9844191814799479|\n",
      "+--------------+---------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relation_status_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9bab101a-590e-434c-a54d-6337092af016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Year|count(Type)|\n",
      "+----+-----------+\n",
      "|1975|          1|\n",
      "|2023|      23409|\n",
      "|1985|          1|\n",
      "|2011|          1|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yearly_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "cb6d6083-4d6a-4db3-869b-bdcad40ef4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----------+\n",
      "|Year|Day_of_month|count(Type)|\n",
      "+----+------------+-----------+\n",
      "|1985|          28|          1|\n",
      "|1975|          23|          1|\n",
      "|2011|          13|          1|\n",
      "|2023|          22|      23409|\n",
      "+----+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day_of_month_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a116b5-a4e1-4e6e-af40-1acc43fe730b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
